{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZFiaRRSQKFkA",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from stats_count import *\n",
    "from grab_weights import grab_attention_weights, text_preprocessing\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import timeit\n",
    "import ripser_count\n",
    "\n",
    "import localfuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d9bW5x0qKFkE"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iRO_jDW-KFkF"
   },
   "outputs": [],
   "source": [
    "!env | grep CUDA_VISIBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xqUWYK9KFkH"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yph-x6_-KFkI"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LYH3c8aVKFkJ"
   },
   "outputs": [],
   "source": [
    "max_tokens_amount  = 128 # The number of tokens to which the tokenized text is truncated / padded.\n",
    "stats_cap          = 500 # Max value that the feature can take. Is NOT applicable to Betty numbers.\n",
    "    \n",
    "layers_of_interest = [i for i in range(12)]  # Layers for which attention matrices and features on them are \n",
    "                                             # calculated. For calculating features on all layers, leave it be\n",
    "                                             # [i for i in range(12)].\n",
    "stats_name = \"s_e_v_c_b0b1\" # The set of topological features that will be count (see explanation below)\n",
    "\n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] # The set of thresholds\n",
    "thrs = len(thresholds_array)                           # (\"t\" in the paper)\n",
    "\n",
    "model_path = tokenizer_path = \"DeepPavlov/rubert-base-cased\"  \n",
    "\n",
    "# You can use either standard or fine-tuned BERT. If you want to use fine-tuned BERT to your current task, save the\n",
    "# model and the tokenizer with the commands tokenizer.save_pretrained(output_dir); \n",
    "# bert_classifier.save_pretrained(output_dir) into the same directory and insert the path to it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agsGLZPyKFkL"
   },
   "source": [
    "### Explanation of stats_name parameter\n",
    "\n",
    "Currently, we implemented calculation of the following graphs features:\n",
    "* \"s\"    - amount of strongly connected components\n",
    "* \"w\"    - amount of weakly connected components\n",
    "* \"e\"    - amount of edges\n",
    "* \"v\"    - average vertex degree\n",
    "* \"c\"    - amount of (directed) simple cycles\n",
    "* \"b0b1\" - Betti numbers\n",
    "\n",
    "The variable stats_name contains a string with the names of the features, which you want to calculate. The format of the string is the following:\n",
    "\n",
    "\"stat_name + \"_\" + stat_name + \"_\" + stat_name + ...\"\n",
    "\n",
    "**For example**:\n",
    "\n",
    "`stats_name == \"s_w\"` means that the number of strongly and weakly connected components will be calculated\n",
    "\n",
    "`stats_name == \"b0b1\"` means that only the Betti numbers will be calculated\n",
    "\n",
    "`stats_name == \"b0b1_c\"` means that Betti numbers and the number of simple cycles will be calculated\n",
    "\n",
    "e.t.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGdL2Lj8KFkN"
   },
   "source": [
    "## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q_ugeC5LKFkP"
   },
   "outputs": [],
   "source": [
    "subset = \"train_ru\"           # .csv file with the texts, for which we count topological features\n",
    "input_dir = \"small_gpt_web/\"  # Name of the directory with .csv file\n",
    "output_dir = \"small_gpt_web/\" # Name of the directory with calculations results\n",
    "\n",
    "prefix = output_dir + subset\n",
    "\n",
    "r_file     = output_dir + 'attentions/' + subset  + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_MAX_LEN_\" + \\\n",
    "             str(max_tokens_amount) + \"_\" + model_path.split(\"/\")[-1]\n",
    "# Name of the file for attention matrices weights\n",
    "\n",
    "stats_file = output_dir + 'features/' + subset + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_\" + stats_name \\\n",
    "             + \"_lists_array_\" + str(thrs) + \"_thrs_MAX_LEN_\" + str(max_tokens_amount) + \\\n",
    "             \"_\" + model_path.split(\"/\")[-1] + '.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wOXKTXv6KFkR",
    "outputId": "eadedd60-0c87-4fa5-d17c-60c22d96df78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small_gpt_web/features/train_ru_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_128_bert-base-uncased.npy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DLKeH0zTKFkT",
    "outputId": "1c79a586-602b-42bc-e303-439c4b8edf10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small_gpt_web/attentions/train_ru_all_heads_12_layers_MAX_LEN_128_bert-base-uncased'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GER-1jnjKFkU"
   },
   "source": [
    ".csv file must contain the column with the name **sentence** with the texts. It can also contain the column **labels**, which will be needed for testing. Any other arbitrary columns will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7ewXcezAKFkV"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(input_dir + subset + \".csv\").reset_index(drop=True)\n",
    "except:\n",
    "    #data = pd.read_csv(input_dir + subset + \".tsv\", delimiter=\"\\t\")\n",
    "    data = pd.read_csv(input_dir + subset + \".tsv\", delimiter=\"\\t\", header=None)\n",
    "    data.columns = [\"0\", \"labels\", \"2\", \"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7ewXcezAKFkV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129066"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7ewXcezAKFkV"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 10 # batch size\n",
    "number_of_batches = ceil(len(data['Text']) / batch_size)\n",
    "DUMP_SIZE = 100 # number of batches to be dumped\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3Y9zHfBKFkW",
    "outputId": "8978b080-ea25-4e95-94b2-e6fa191e9240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of words in example: 222.06571831466073\n",
      "Max. amount of words in example: 3484\n",
      "Min. amount of words in example: 5\n"
     ]
    }
   ],
   "source": [
    "sentences = data['Text']\n",
    "print(\"Average amount of words in example:\", \\\n",
    "      np.mean(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['Text'])))))\n",
    "print(\"Max. amount of words in example:\", \\\n",
    "      np.max(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['Text'])))))\n",
    "print(\"Min. amount of words in example:\", \\\n",
    "      np.min(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['Text'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3Y9zHfBKFkW",
    "outputId": "8978b080-ea25-4e95-94b2-e6fa191e9240"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = max_tokens_amount\n",
    "\n",
    "import pickle\n",
    "# tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "# Our computing cluster does not allow internet connections from jupyter. Hence tokenizer is loaded from login shell and serialized.\n",
    "f = open('/home/amshtareva/tokenizer_obj','rb')\n",
    "tokenizer = pickle.load(f)\n",
    "#tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "a53jVWm3KFkW"
   },
   "outputs": [],
   "source": [
    "def get_token_length(batch_texts):\n",
    "    inputs = tokenizer.batch_encode_plus(batch_texts,\n",
    "       return_tensors='pt',\n",
    "       add_special_tokens=True,\n",
    "       max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "       pad_to_max_length=True,         # Pad sentence to max length\n",
    "       truncation=True\n",
    "    )\n",
    "    inputs = inputs['input_ids'].numpy()\n",
    "    n_tokens = []\n",
    "    indexes = np.argwhere(inputs == tokenizer.pad_token_id)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        ids = indexes[(indexes == i)[:, 0]]\n",
    "        if not len(ids):\n",
    "            n_tokens.append(MAX_LEN)\n",
    "        else:\n",
    "            n_tokens.append(ids[0, 1])\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mHzEfZBKFkY"
   },
   "outputs": [],
   "source": [
    "data['tokenizer_length'] = get_token_length(data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "TU5tHwWmKFkZ",
    "outputId": "757b6d74-45f2-455b-c970-2a5ab98f186b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzsAuJX0KFkZ"
   },
   "outputs": [],
   "source": [
    "ntokens_array = data['tokenizer_length'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzsAuJX0KFkZ"
   },
   "outputs": [],
   "source": [
    "pickle.dump(ntokens_array, open('ntokens.obj', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iYp6-39KFka"
   },
   "source": [
    "## Attention extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch8NpZw8KFka"
   },
   "source": [
    "Loading **BERT** and tokenizers using **transformers** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7v5_TaHKFka"
   },
   "outputs": [],
   "source": [
    "batched_sentences = np.array_split(data['Text'].values, number_of_batches)\n",
    "adj_matricies = []\n",
    "adj_filenames = []\n",
    "batch_size = 10 # batch size\n",
    "number_of_batches = ceil(len(data['Text']) / batch_size)\n",
    "DUMP_SIZE = 100 # number of batches to be dumped\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)\n",
    "number_of_batches_single = ceil(len(data['Text']) / batch_size)\n",
    "single_set = ceil(number_of_batches_single / DUMP_SIZE)\n",
    "assert number_of_batches == len(batched_sentences) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7v5_TaHKFka"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95l3Vs6OKFkb",
    "outputId": "e257cf73-7535-48b5-add5-c206d9a8882f"
   },
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)\n",
    "g = open('/home/amshtareva/model_obj','rb')\n",
    "model = pickle.load(g)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95l3Vs6OKFkb",
    "outputId": "e257cf73-7535-48b5-add5-c206d9a8882f"
   },
   "outputs": [],
   "source": [
    "template_features_array = pickle.load(open('template_features_array.obj', 'rb'))\n",
    "stats_tuple_lists_array = pickle.load(open('stats_tuple_lists_array.obj', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95l3Vs6OKFkb",
    "outputId": "e257cf73-7535-48b5-add5-c206d9a8882f"
   },
   "outputs": [],
   "source": [
    "mod = 2000\n",
    "number_of_batches_single = ceil(mod / batch_size)\n",
    "single_set = ceil(number_of_batches_single / DUMP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95l3Vs6OKFkb",
    "outputId": "e257cf73-7535-48b5-add5-c206d9a8882f"
   },
   "outputs": [],
   "source": [
    "feature_list = ['self', 'beginning', 'prev', 'next', 'comma', 'dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95l3Vs6OKFkb",
    "outputId": "e257cf73-7535-48b5-add5-c206d9a8882f"
   },
   "outputs": [],
   "source": [
    "#attention_dir = 'small_gpt_web/attentions/'\n",
    "#attention_name = subset + '_all_heads_12_layers_MAX_LEN_128_bert-base-uncased'\n",
    "\n",
    "#texts_name = 'small_gpt_web/' + subset + '.csv'\n",
    "\n",
    "MAX_LEN = 128\n",
    "component = 64\n",
    "iterv = number_of_batches - number_of_batches_single*component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adgONzWtKFkc",
    "outputId": "29cce1a1-58c9-45ff-9d6c-d565f479c0d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while iterv > 0:\n",
    "#for component in range(4):\n",
    "    for i in tqdm(range(min(number_of_batches_single, iterv)), desc=\"Weights calc\"): \n",
    "        attention_w = grab_attention_weights(model, tokenizer, batched_sentences[i+component*number_of_batches_single], max_tokens_amount, device)\n",
    "        # sample X layer X head X n_token X n_token\n",
    "        adj_matricies.append(attention_w)\n",
    "        if (i+1) % DUMP_SIZE == 0: # dumping\n",
    "            print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "            adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "            print(\"Concatenated\")\n",
    "            adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "            # Carefully with boundaries\n",
    "            filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)+component*single_set) + \"of\" + str(number_of_files) + '.npy'\n",
    "            print(f\"Saving weights to : {filename}\")\n",
    "            adj_filenames.append(filename)\n",
    "            np.save(filename, adj_matricies)\n",
    "            adj_matricies = []\n",
    "\n",
    "    if len(adj_matricies):\n",
    "        print(\"Alert!\")\n",
    "        filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)+component*single_set) + \"of\" + str(number_of_files) + '.npy'\n",
    "        print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "        adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "        print(\"Concatenated\")\n",
    "        adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "        print(f\"Saving weights to : {filename}\")\n",
    "        np.save(filename, adj_matricies)\n",
    "        adj_matricies = []\n",
    "\n",
    "    print(\"Results saved.\")\n",
    "    \n",
    "    adj_filenames = [\n",
    "        output_dir + 'attentions/' + filename \n",
    "        for filename in os.listdir(output_dir + 'attentions/') if r_file in (output_dir + 'attentions/' + filename)\n",
    "    ]\n",
    "    # sorted by part number\n",
    "    adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "    print(adj_filenames)\n",
    "    \n",
    "    for i, filename in enumerate(tqdm(adj_filenames, desc='Вычисление признаков')):\n",
    "        adj_matricies = np.load(filename, allow_pickle=True)\n",
    "        print((i+component*single_set)*batch_size*DUMP_SIZE)\n",
    "        print((i+component*single_set+1)*batch_size*DUMP_SIZE)\n",
    "        ntokens = ntokens_array[(i+component*single_set)*batch_size*DUMP_SIZE : (i+component*single_set+1)*batch_size*DUMP_SIZE]\n",
    "        splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers) # 2 or 20.\n",
    "        args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "        stats_tuple_lists_array_part = pool.starmap(\n",
    "            count_top_stats, args\n",
    "        )\n",
    "        stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))\n",
    "    \n",
    "    for i, filename in tqdm(list(enumerate(adj_filenames)), desc='Features calc'):\n",
    "        adj_matricies = np.load(filename, allow_pickle=True)\n",
    "        ntokens = ntokens_array[(i+component*single_set)*batch_size*DUMP_SIZE : (i+component*single_set+1)*batch_size*DUMP_SIZE]\n",
    "        splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "        \n",
    "        args = [(m, feature_list, list_of_ids) for m, list_of_ids in splitted]\n",
    "\n",
    "        template_features_array_part = pool.starmap(\n",
    "            calculate_features_t, args\n",
    "        )\n",
    "        template_features_array.append(np.concatenate([_ for _ in template_features_array_part], axis=3))\n",
    "\n",
    "    iterv = iterv - number_of_batches_single\n",
    "    component = component + 1\n",
    "    \n",
    "    for filename in os.listdir(output_dir + 'attentions/'):\n",
    "        filepath = os.path.join(output_dir + 'attentions/', filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "            \n",
    "    pickle.dump(stats_tuple_lists_array, open('stats_tuple_lists_array.obj', 'wb'))\n",
    "    pickle.dump(template_features_array, open('template_features_array.obj', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9P5xYS0KFkc"
   },
   "source": [
    "## Calculating topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbMi7RrJKFke"
   },
   "outputs": [],
   "source": [
    "stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQtFyTq7KFke",
    "outputId": "0f887510-35e3-4b1f-a4db-2fe7f45878e1"
   },
   "outputs": [],
   "source": [
    "stats_tuple_lists_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0IBUaOQKFkf",
    "outputId": "d1d185bd-f684-49f9-9ccd-92e6019294d4"
   },
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == -inf]) + \\\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-vIk-dtqKFkg",
    "outputId": "b1d68a9c-561b-4e27-de72-ddc20fd02b98"
   },
   "outputs": [],
   "source": [
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSrA5h9NKFkg"
   },
   "outputs": [],
   "source": [
    "np.save(stats_file, stats_tuple_lists_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0X1_k-MKFkg"
   },
   "source": [
    "##### Checking the size of features matrices:\n",
    "\n",
    "Layers amount **Х** Heads amount **Х** Features amount **X** Examples amount **Х** Thresholds amount\n",
    "\n",
    "**For example**:\n",
    "\n",
    "`stats_name == \"s_w\"` => `Features amount == 2`\n",
    "\n",
    "`stats_name == \"b0b1\"` => `Features amount == 2`\n",
    "\n",
    "`stats_name == \"b0b1_c\"` => `Features amount == 3`\n",
    "\n",
    "e.t.c.\n",
    "\n",
    "`thresholds_array == [0.025, 0.05, 0.1, 0.25, 0.5, 0.75]` => `Thresholds amount == 6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVCOV8i7KFkh"
   },
   "outputs": [],
   "source": [
    "template_features_array = np.concatenate(template_features_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVCOV8i7KFkh"
   },
   "outputs": [],
   "source": [
    "\"small_gpt_web/features/\" + attention_name + \"_template.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVCOV8i7KFkh"
   },
   "outputs": [],
   "source": [
    "np.save(\"small_gpt_web/features/\" + attention_name + \"_template.npy\", template_features_array)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "features_calculation_by_thresholds.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tda]",
   "language": "python",
   "name": "conda-env-.conda-tda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
